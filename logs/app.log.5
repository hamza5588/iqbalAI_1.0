2025-09-09 02:48:18,266 INFO: Versions data: [{'id': 182, 'teacher_id': 12, 'title': 'ragg', 'summary': 'This lesson explores the application of Long Short-Term Memory (LSTM) networks with self-attention mechanisms to model variations in running pace. It discusses the fundamentals of LSTM and self-attention, their integration, and their application to running pace modeling. The lesson aims to equip learners with the knowledge to design and implement such models for real-time adjustments in running workouts.', 'learning_objectives': 'ai', 'focus_area': 'Other', 'grade_level': 'College', 'content': 'LSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'file_name': 'Employing_an_LSTM_model_with_a_self-attention_mech (1).pdf', 'created_at': '2025-09-08 21:11:16', 'updated_at': '2025-09-08 21:11:16', 'is_public': 1, 'parent_lesson_id': None, 'version': 1, 'draft_content': None, 'lesson_id': 'L1757365876203', 'version_number': 1, 'parent_version_id': None, 'original_content': 'LSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'status': 'finalized', 'has_child_version': 0, 'teacher_name': 'hamza5588', 'is_original': True}] [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:477]
2025-09-09 02:48:30,090 INFO: Retrieved lesson ID 182 with title: 'ragg' [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:239]
2025-09-09 02:48:30,091 INFO: Answering question for lesson 'ragg' (ID: 182) [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\services\lesson_service.py:1135]
2025-09-09 02:52:34,155 INFO: Checking survey submission status for user_id: 11 [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:1544]
2025-09-09 02:52:34,158 INFO: Survey check result: User 11 has not submitted a survey (count: 0) [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:1553]
2025-09-09 02:52:52,406 INFO: Retrieved lesson ID 182 with title: 'ragg' [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:239]
2025-09-09 02:52:52,408 INFO: get_lesson_versions for lesson 182: found 1 versions [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:476]
2025-09-09 02:52:52,409 INFO: Versions data: [{'id': 182, 'teacher_id': 12, 'title': 'ragg', 'summary': 'This lesson explores the application of Long Short-Term Memory (LSTM) networks with self-attention mechanisms to model variations in running pace. It discusses the fundamentals of LSTM and self-attention, their integration, and their application to running pace modeling. The lesson aims to equip learners with the knowledge to design and implement such models for real-time adjustments in running workouts.', 'learning_objectives': 'ai', 'focus_area': 'Other', 'grade_level': 'College', 'content': 'LSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'file_name': 'Employing_an_LSTM_model_with_a_self-attention_mech (1).pdf', 'created_at': '2025-09-08 21:11:16', 'updated_at': '2025-09-08 21:11:16', 'is_public': 1, 'parent_lesson_id': None, 'version': 1, 'draft_content': None, 'lesson_id': 'L1757365876203', 'version_number': 1, 'parent_version_id': None, 'original_content': 'LSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'status': 'finalized', 'has_child_version': 0, 'teacher_name': 'hamza5588', 'is_original': True}] [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:477]
2025-09-09 02:53:03,099 INFO: Retrieved lesson ID 182 with title: 'ragg' [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:239]
2025-09-09 02:53:03,101 INFO: get_lesson_versions for lesson 182: found 1 versions [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:476]
