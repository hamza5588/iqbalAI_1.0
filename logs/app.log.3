2025-09-09 03:00:11,093 INFO: Versions data: [{'id': 182, 'teacher_id': 12, 'title': 'ragg', 'summary': 'This lesson explores the application of Long Short-Term Memory (LSTM) networks with self-attention mechanisms to model variations in running pace. It discusses the fundamentals of LSTM and self-attention, their integration, and their application to running pace modeling. The lesson aims to equip learners with the knowledge to design and implement such models for real-time adjustments in running workouts.', 'learning_objectives': 'ai', 'focus_area': 'Other', 'grade_level': 'College', 'content': 'LSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'file_name': 'Employing_an_LSTM_model_with_a_self-attention_mech (1).pdf', 'created_at': '2025-09-08 21:11:16', 'updated_at': '2025-09-08 21:11:16', 'is_public': 1, 'parent_lesson_id': None, 'version': 1, 'draft_content': None, 'lesson_id': 'L1757365876203', 'version_number': 1, 'parent_version_id': None, 'original_content': 'LSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'status': 'finalized', 'has_child_version': 0, 'teacher_name': 'hamza5588', 'is_original': True}] [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:477]
2025-09-09 03:00:25,331 INFO: Retrieved lesson ID 175 with title: 'rag update' [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:239]
2025-09-09 03:00:25,332 INFO: get_lesson_versions for lesson 175: found 1 versions [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:476]
2025-09-09 03:00:25,333 INFO: Versions data: [{'id': 175, 'teacher_id': 12, 'title': 'rag update', 'summary': 'This lesson explores the application of a Self-Attention LSTM model to fit variations in running pace, utilizing metrics such as cadence, stride length, and pace. The model integrates the advantages of both methodologies, allowing for improved learning of relationships between data points. Experimental results demonstrate the effectiveness of the Self-Attention-LSTM model in processing time series data and its potential for real-time feedback on running performance.', 'learning_objectives': 'ai', 'focus_area': 'Other', 'grade_level': '9-12', 'content': "## Introduction to Self-Attention LSTM Models\nSelf-Attention LSTM models are a type of recurrent neural network that combines the strengths of LSTM and self-attention mechanisms. The self-attention mechanism enables the model to consider direct connections between different parts of a sequence, while the LSTM method captures long-term dependencies in the data. This integration allows for improved learning of relationships between data points and enhances the model's ability to process complex sequential data.\n\n## Application of Self-Attention LSTM Models\nThe Self-Attention LSTM model is employed to fit variations in running pace using metrics such as cadence, stride length, and pace. The model takes these metrics as inputs and uses self-attention mechanisms to capture intrinsic relationships within the feature sequences. The LSTM method is then used to understand inter-feature relationships, enabling real-time feedback on data values at any given moment. Experimental results demonstrate the effectiveness of the Self-Attention-LSTM model in processing time series data and its potential for improving running performance.\n\n## Future Directions and Potential Applications\nThe Self-Attention LSTM model has shown promising results in fitting variations in running pace and has the potential for real-time feedback on running performance. Future directions include exploring the application of this model in other sports and activities, as well as investigating the use of other machine learning techniques to improve the accuracy and effectiveness of the model.", 'file_name': 'Employing_an_LSTM_model_with_a_self-attention_mech (1).pdf', 'created_at': '2025-09-08 20:38:43', 'updated_at': '2025-09-08 20:40:05', 'is_public': 1, 'parent_lesson_id': 174, 'version': 1, 'draft_content': None, 'lesson_id': 'L1757363886798', 'version_number': 2, 'parent_version_id': 174, 'original_content': "## Introduction to Self-Attention LSTM Models\nSelf-Attention LSTM models are a type of recurrent neural network that combines the strengths of LSTM and self-attention mechanisms. The self-attention mechanism enables the model to consider direct connections between different parts of a sequence, while the LSTM method captures long-term dependencies in the data. This integration allows for improved learning of relationships between data points and enhances the model's ability to process complex sequential data.\n\n## Application of Self-Attention LSTM Models\nThe Self-Attention LSTM model is employed to fit variations in running pace using metrics such as cadence, stride length, and pace. The model takes these metrics as inputs and uses self-attention mechanisms to capture intrinsic relationships within the feature sequences. The LSTM method is then used to understand inter-feature relationships, enabling real-time feedback on data values at any given moment. Experimental results demonstrate the effectiveness of the Self-Attention-LSTM model in processing time series data and its potential for improving running performance.\n\n## Future Directions and Potential Applications\nThe Self-Attention LSTM model has shown promising results in fitting variations in running pace and has the potential for real-time feedback on running performance. Future directions include exploring the application of this model in other sports and activities, as well as investigating the use of other machine learning techniques to improve the accuracy and effectiveness of the model.", 'status': 'finalized', 'has_child_version': 1, 'teacher_name': 'hamza5588'}] [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:477]
2025-09-09 03:00:48,239 INFO: Retrieved lesson ID 170 with title: 'rag' [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:239]
2025-09-09 03:00:48,240 INFO: get_lesson_versions for lesson 170: found 1 versions [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:476]
