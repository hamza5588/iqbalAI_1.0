2025-09-13 01:20:40,425 INFO: Versions data: [{'id': 182, 'teacher_id': 12, 'title': 'ragg', 'summary': 'This lesson explores the application of Long Short-Term Memory (LSTM) networks with self-attention mechanisms to model variations in running pace. It discusses the fundamentals of LSTM and self-attention, their integration, and their application to running pace modeling. The lesson aims to equip learners with the knowledge to design and implement such models for real-time adjustments in running workouts.', 'learning_objectives': 'ai', 'focus_area': 'Other', 'grade_level': 'College', 'content': 'LSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'file_name': 'Employing_an_LSTM_model_with_a_self-attention_mech (1).pdf', 'created_at': '2025-09-08 21:11:16', 'updated_at': '2025-09-12 20:16:41', 'is_public': 1, 'parent_lesson_id': None, 'version': 1, 'draft_content': None, 'lesson_id': 'L1757365876203', 'version_number': 1, 'parent_version_id': None, 'original_content': 'LSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'status': 'finalized', 'has_child_version': 1, 'teacher_name': 'hamza5588', 'is_original': True}, {'id': 183, 'teacher_id': 12, 'title': 'ragg', 'summary': 'This lesson explores the application of Long Short-Term Memory (LSTM) networks with self-attention mechanisms to model variations in running pace. It discusses the fundamentals of LSTM and self-attention, their integration, and their application to running pace modeling. The lesson aims to equip learners with the knowledge to design and implement such models for real-time adjustments in running workouts.', 'learning_objectives': 'ai', 'focus_area': 'Other', 'grade_level': 'College', 'content': '## Introduction to LSTM Networks and Self-Attention Mechanisms\nLSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\n## Architecture of the Self-Attention LSTM Model\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\n## Conclusion and Future Applications\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'file_name': 'Employing_an_LSTM_model_with_a_self-attention_mech (1).pdf', 'created_at': '2025-09-12 20:16:41', 'updated_at': '2025-09-12 20:16:41', 'is_public': 1, 'parent_lesson_id': 182, 'version': 1, 'draft_content': None, 'lesson_id': 'L1757365876203', 'version_number': 2, 'parent_version_id': 182, 'original_content': '## Introduction to LSTM Networks and Self-Attention Mechanisms\nLSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\n## Architecture of the Self-Attention LSTM Model\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\n## Conclusion and Future Applications\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'status': 'finalized', 'has_child_version': 0, 'teacher_name': 'hamza5588', 'is_original': False}] [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:477]
2025-09-13 01:20:40,990 INFO: Retrieved lesson ID 182 with title: 'ragg' [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:239]
2025-09-13 01:20:44,317 INFO: Retrieved lesson ID 183 with title: 'ragg' [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:239]
2025-09-13 01:20:44,318 INFO: get_lesson_versions for lesson 183: found 1 versions [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:476]
2025-09-13 01:20:44,319 INFO: Versions data: [{'id': 183, 'teacher_id': 12, 'title': 'ragg', 'summary': 'This lesson explores the application of Long Short-Term Memory (LSTM) networks with self-attention mechanisms to model variations in running pace. It discusses the fundamentals of LSTM and self-attention, their integration, and their application to running pace modeling. The lesson aims to equip learners with the knowledge to design and implement such models for real-time adjustments in running workouts.', 'learning_objectives': 'ai', 'focus_area': 'Other', 'grade_level': 'College', 'content': '## Introduction to LSTM Networks and Self-Attention Mechanisms\nLSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\n## Architecture of the Self-Attention LSTM Model\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\n## Conclusion and Future Applications\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'file_name': 'Employing_an_LSTM_model_with_a_self-attention_mech (1).pdf', 'created_at': '2025-09-12 20:16:41', 'updated_at': '2025-09-12 20:16:41', 'is_public': 1, 'parent_lesson_id': 182, 'version': 1, 'draft_content': None, 'lesson_id': 'L1757365876203', 'version_number': 2, 'parent_version_id': 182, 'original_content': '## Introduction to LSTM Networks and Self-Attention Mechanisms\nLSTM networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies in sequential data. Self-attention mechanisms allow models to consider direct relationships between different parts of a sequence, enhancing performance. This section introduces these concepts and their relevance to modeling running pace variations.\n\n## Architecture of the Self-Attention LSTM Model\nThis section delves into the architecture of the Self-Attention LSTM model, including the input layer, LSTM layer, self-attention layer, and output layer. It explains how each component contributes to learning the relationships between stride length, frequency, and pace, enabling the model to predict running pace variations accurately.\n\n## Conclusion and Future Applications\nThe conclusion summarizes the key points of the lesson, emphasizing the potential of the Self-Attention LSTM model in sports analytics, particularly in running. It touches upon future applications and the importance of such models in enhancing training effectiveness and personalized coaching.', 'status': 'finalized', 'has_child_version': 0, 'teacher_name': 'hamza5588'}] [in C:\Users\DCS\Desktop\New folder (10)\iqbalAI_1.0\app\models\models.py:477]
